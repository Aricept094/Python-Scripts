{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "def fetch_abstract_from_pubmed(pmid):\n",
    "    \"\"\"\n",
    "    Fetches the abstract of a PubMed article given its PMID using NCBI E-utilities.\n",
    "    Retrieves FULL XML record and extracts abstract text from <Abstract> and <AbstractText> tags.\n",
    "    Handles nested tags within <AbstractText> using .itertext().\n",
    "    Returns abstract text if found, None otherwise.  Logs detailed debug information.\n",
    "\n",
    "    Note: Abstracts may not be available in XML for all PubMed article types (e.g., Letters).\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Fetching abstract (FULL XML) for PMID: {pmid}\")\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"retmode\": \"xml\",\n",
    "        \"rettype\": \"xml\",  # Get full XML to maximize abstract retrieval\n",
    "        \"id\": pmid\n",
    "    }\n",
    "    abstract_text = None\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        xml_content = response.text\n",
    "        root = ET.fromstring(xml_content)\n",
    "\n",
    "        abstract_element = root.find('.//Abstract') # Search for <Abstract> tag\n",
    "        if abstract_element is not None:\n",
    "            abstract_parts = []\n",
    "            for abstract_text_element in abstract_element.findall('.//AbstractText'): # Find all <AbstractText> tags\n",
    "                abstract_parts.append(\"\".join(abstract_text_element.itertext())) # Extract all text using .itertext()\n",
    "            abstract_text = \"\\n\".join(part for part in abstract_parts if part) # Join parts with newline\n",
    "            if abstract_text:\n",
    "                print(f\"DEBUG: Abstract extracted for PMID: {pmid}\")\n",
    "            else:\n",
    "                print(f\"DEBUG: <Abstract> tag found for PMID: {pmid}, but no content in <AbstractText>.\")\n",
    "        else:\n",
    "            print(f\"DEBUG: <Abstract> tag NOT found for PMID: {pmid}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ERROR: Network error fetching PMID {pmid}: {e}\")\n",
    "        return None\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"ERROR: XML Parse error for PMID {pmid}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Unexpected error processing PMID {pmid}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        print(f\"DEBUG: Finished fetching abstract for PMID: {pmid}\")\n",
    "        return abstract_text\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Reads article data from CSV, fetches abstracts from PubMed for each PMID, and saves\n",
    "    to a new CSV. Uses FULL XML retrieval for improved abstract capture.\n",
    "    Prints cumulative count of collected abstracts after each article.\n",
    "    Saves intermediate CSV every N steps.\n",
    "    \"\"\"\n",
    "    input_csv_file = \"/home/aricept094/mydata/csv-DigitalHea-set.csv\"\n",
    "    output_csv_file = \"pubmed_articles_with_abstracts_final.csv\" # Final output file name\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(f\"INFO: Starting script at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
    "    print(f\"INFO: Reading article data from {input_csv_file}...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_file)\n",
    "        total_articles = len(df)\n",
    "        print(f\"INFO: Loaded data for {total_articles} articles.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input CSV file not found at {input_csv_file}\")\n",
    "        return\n",
    "\n",
    "    abstracts = [None] * total_articles # Initialize abstracts list with None, same length as articles\n",
    "    successful_abstract_count = 0 # Counter for successful abstracts\n",
    "    processed_count = 0\n",
    "    save_interval = 200 # Save intermediate progress every 10 articles (set to 10 as requested)\n",
    "\n",
    "    print(f\"INFO: Fetching abstracts from PubMed...\")\n",
    "    for index, row in df.iterrows():\n",
    "        pmid = str(row['PMID'])\n",
    "        print(f\"INFO: Processing article {processed_count + 1}/{total_articles}, PMID: {pmid}...\")\n",
    "        abstract = fetch_abstract_from_pubmed(pmid)\n",
    "        abstracts[index] = abstract # Assign abstract to the correct index\n",
    "        if abstract: # Increment counter if abstract is successfully fetched (not None)\n",
    "            successful_abstract_count += 1\n",
    "        processed_count += 1\n",
    "\n",
    "        print(f\"INFO: Abstract processed for PMID: {pmid}. {total_articles - processed_count} articles remaining. Total abstracts collected: {successful_abstract_count}\") # Added print for total abstracts\n",
    "\n",
    "        time.sleep(0.1) # Be respectful to NCBI servers\n",
    "\n",
    "        if processed_count % save_interval == 0:\n",
    "            print(f\"INFO: Saving intermediate progress after {processed_count} articles...\")\n",
    "            df_intermediate = df.copy() # Create a copy to avoid modifying original during iteration\n",
    "            df_intermediate['Abstract'] = abstracts # Assign the current abstracts list\n",
    "            intermediate_output_file = f\"pubmed_articles_with_abstracts_intermediate_{processed_count}.csv\"\n",
    "            try:\n",
    "                df_intermediate.to_csv(intermediate_output_file, index=False)\n",
    "                print(f\"INFO: Saved intermediate data to {intermediate_output_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Error saving intermediate CSV file: {e}\")\n",
    "            del df_intermediate # Clean up intermediate DataFrame\n",
    "\n",
    "    df['Abstract'] = abstracts # Final assignment of abstracts to DataFrame\n",
    "\n",
    "    print(f\"INFO: Abstract fetching complete.\")\n",
    "    print(f\"INFO: Saving final results to {output_csv_file}...\")\n",
    "    try:\n",
    "        df.to_csv(output_csv_file, index=False)\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        print(f\"SUCCESS: Saved articles with abstracts to {output_csv_file}\")\n",
    "        print(f\"INFO: Script finished at {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}\")\n",
    "        print(f\"INFO: Total processing time: {duration:.2f} seconds\")\n",
    "        print(f\"INFO: Successfully retrieved abstracts for {successful_abstract_count}/{total_articles} articles.\") # Report success count\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error saving final output CSV file: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
