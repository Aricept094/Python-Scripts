{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting PDF...\n",
      "PDF split into 64 chunks.\n",
      "Converting to Markdown...\n",
      "Merging Markdown files...\n",
      "Merged Markdown file created at: /home/aricept094/mydata/merged_output.md\n",
      "Cleaning up temporary files...\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "from PyPDF2 import PdfReader\n",
    "from markitdown import MarkItDown\n",
    "import multiprocessing\n",
    "\n",
    "def split_pdf_efficiently(pdf_path, num_chunks):\n",
    "    \"\"\"Splits a PDF into approximately equal-sized chunks using pdftk efficiently.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file.\n",
    "        num_chunks: The desired number of output chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of paths to the generated PDF chunks.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    num_pages = len(reader.pages)\n",
    "    pages_per_chunk = num_pages // num_chunks\n",
    "    chunk_paths = []\n",
    "    output_dir = os.path.dirname(pdf_path)\n",
    "\n",
    "    # Build a list of page ranges for pdftk\n",
    "    page_ranges = []\n",
    "    start_page = 1\n",
    "    for i in range(num_chunks):\n",
    "        end_page = start_page + pages_per_chunk -1\n",
    "        if i == num_chunks - 1: #last chunk\n",
    "           end_page = num_pages\n",
    "        page_ranges.append((start_page, end_page))\n",
    "        start_page = end_page + 1\n",
    "\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for i, (start, end) in enumerate(page_ranges):\n",
    "            output_filename = f\"chunk_{i}_{start}-{end}.pdf\"\n",
    "            output_filepath = os.path.join(output_dir, output_filename)\n",
    "            chunk_paths.append(output_filepath)\n",
    "\n",
    "            future = executor.submit(run_pdftk, pdf_path, start, end, output_filepath)\n",
    "            futures.append(future)\n",
    "\n",
    "        #Wait all and check exceptions.\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                future.result() #get exception if exists.\n",
    "            except Exception as e:\n",
    "                print(f\"Error during PDF splitting: {e}\")\n",
    "\n",
    "\n",
    "    return chunk_paths\n",
    "\n",
    "def run_pdftk(pdf_path, start_page, end_page, output_path):\n",
    "    \"\"\"Runs pdftk to extract a specific page range.\"\"\"\n",
    "    command = [\n",
    "        \"pdftk\",\n",
    "        pdf_path,\n",
    "        \"cat\",\n",
    "        f\"{start_page}-{end_page}\",\n",
    "        \"output\",\n",
    "        output_path\n",
    "    ]\n",
    "    subprocess.run(command, check=True, capture_output=True)\n",
    "\n",
    "\n",
    "def convert_chunk(chunk_path, docintel_endpoint=None):\n",
    "    \"\"\"Converts a single PDF chunk to Markdown.\"\"\"\n",
    "    try:\n",
    "        output_md_path = os.path.splitext(chunk_path)[0] + \".md\"\n",
    "        md = MarkItDown(docintel_endpoint=docintel_endpoint) if docintel_endpoint else MarkItDown()\n",
    "        result = md.convert(chunk_path)\n",
    "        with open(output_md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(result.text_content)\n",
    "        return output_md_path, None\n",
    "    except Exception as e:\n",
    "        return None, (chunk_path, e)\n",
    "\n",
    "\n",
    "def merge_markdown_files(markdown_files, output_file):\n",
    "    \"\"\"Merges multiple Markdown files.\"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for filename in markdown_files:\n",
    "            try:\n",
    "                with open(filename, \"r\", encoding=\"utf-8\") as infile:\n",
    "                    outfile.write(infile.read())\n",
    "                outfile.write(\"\\n\\n---\\n\\n\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"Warning: {filename} not found, skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading or writing during merge: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    pdf_file = \"/home/aricept094/mydata/Nelson Textbook of Pediatrics, 21th edition 2020.pdf\"\n",
    "    output_dir = os.path.dirname(pdf_file)\n",
    "    final_output_file = os.path.join(output_dir, \"merged_output.md\")\n",
    "    docintel_endpoint = None\n",
    "    num_chunks = multiprocessing.cpu_count() * 4  # Adjust - start with 4x your CPU count\n",
    "\n",
    "    # 1. Split the PDF *efficiently*\n",
    "    print(\"Splitting PDF...\")\n",
    "    chunk_paths = split_pdf_efficiently(pdf_file, num_chunks)\n",
    "    print(f\"PDF split into {len(chunk_paths)} chunks.\")\n",
    "\n",
    "    # 2. Convert to Markdown (using multiprocessing.Pool)\n",
    "    print(\"Converting to Markdown...\")\n",
    "    markdown_files = []\n",
    "    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:\n",
    "        results = pool.starmap(convert_chunk, [(path, docintel_endpoint) for path in chunk_paths])\n",
    "        for md_path, error in results:\n",
    "            if md_path:\n",
    "                markdown_files.append(md_path)\n",
    "            elif error:\n",
    "                chunk_path, e = error\n",
    "                print(f\"Error converting {chunk_path}: {e}\")\n",
    "\n",
    "    # 3. Merge\n",
    "    print(\"Merging Markdown files...\")\n",
    "    merge_markdown_files(markdown_files, final_output_file)\n",
    "    print(f\"Merged Markdown file created at: {final_output_file}\")\n",
    "\n",
    "    # 4. Clean up\n",
    "    print(\"Cleaning up temporary files...\")\n",
    "    for chunk_path in chunk_paths:\n",
    "        try:\n",
    "            os.remove(chunk_path)\n",
    "            md_chunk_path = os.path.splitext(chunk_path)[0] + \".md\"\n",
    "            os.remove(md_chunk_path)\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting file {chunk_path}: {e}\")\n",
    "    print(\"Cleanup complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split '/home/aricept094/mydata/merged_output.md' into '/home/aricept094/mydata/merged_output_part1.md' and '/home/aricept094/mydata/merged_output_part2.md'\n",
      "Part 1 size: 13.25 MB\n",
      "Part 2 size: 13.17 MB\n",
      "Total execution time: 0.1683 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def split_markdown_file_balanced(filepath, target_size_mb=10):\n",
    "    \"\"\"\n",
    "    Splits a markdown file into two files, aiming for more balanced sizes.\n",
    "\n",
    "    Args:\n",
    "        filepath: The path to the markdown file.\n",
    "        target_size_mb: The approximate target size for each part (in MB).\n",
    "                         This is a *target*; the split won't be perfectly even.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {filepath}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {filepath} - {e}\")\n",
    "        return\n",
    "\n",
    "    total_size_bytes = len(content.encode('utf-8'))  # Get size in bytes\n",
    "    target_size_bytes = target_size_mb * 1024 * 1024  # Convert MB to bytes\n",
    "\n",
    "    # Find a good split point near the middle, but on a line break.\n",
    "    split_index = total_size_bytes // 2\n",
    "    best_split_index = -1\n",
    "\n",
    "    # Adjust to find a line break closest to the target size\n",
    "    if split_index < target_size_bytes :\n",
    "        # Search forward for a line break\n",
    "        for i in range(split_index, len(content)):\n",
    "             if content[i] == '\\n':\n",
    "                best_split_index = i\n",
    "                break\n",
    "    else:\n",
    "            #seach backward\n",
    "        for i in range(split_index,0,-1):\n",
    "             if content[i] == '\\n':\n",
    "                best_split_index = i\n",
    "                break\n",
    "\n",
    "    if best_split_index == -1:\n",
    "        print(\"Error: Could not find a suitable split point (no line breaks).\")\n",
    "        return\n",
    "\n",
    "\n",
    "    part1 = content[:best_split_index]\n",
    "    part2 = content[best_split_index:]\n",
    "\n",
    "    basename = os.path.splitext(filepath)[0]\n",
    "    ext = os.path.splitext(filepath)[1]\n",
    "    output_file1 = f\"{basename}_part1{ext}\"\n",
    "    output_file2 = f\"{basename}_part2{ext}\"\n",
    "\n",
    "    try:\n",
    "        with open(output_file1, 'w', encoding='utf-8') as f:\n",
    "            f.write(part1)\n",
    "        with open(output_file2, 'w', encoding='utf-8') as f:\n",
    "            f.write(part2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing output files: {e}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Successfully split '{filepath}' into '{output_file1}' and '{output_file2}'\")\n",
    "    print(f\"Part 1 size: {os.path.getsize(output_file1) / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Part 2 size: {os.path.getsize(output_file2) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    filepath = \"/home/aricept094/mydata/merged_output.md\"\n",
    "    target_size_mb = 10  # Adjust as needed\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use multiprocessing to avoid blocking the main thread\n",
    "    process = multiprocessing.Process(target=split_markdown_file_balanced, args=(filepath, target_size_mb))\n",
    "    process.start()\n",
    "    process.join()  # Wait for the process to complete\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
